{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app_dir = os.path.join(os.getcwd(), \"app\")\n",
    "load_dotenv(os.path.join(app_dir, \".env\"))\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/restaurant.txt'}, page_content='In the charming streets of Palermo, tucked away in a quaint alley, stood Chef Amico, a restaurant that was more than a mere eatery—it was a slice of Sicilian heaven. Founded by Amico, a chef whose name was synonymous with passion and creativity, the restaurant was a mosaic of his life’s journey through the flavors of Italy.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS expects a file_name dict as key\n",
    "for document in chunks:\n",
    "    document.metadata[\"file_name\"] = document.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ragas.testset.evolutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtestset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtestset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple, reasoning, multi_context\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ragas.testset.evolutions'"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    embeddings=embeddings, generator_llm=model, critic_llm=model\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    chunks,\n",
    "    test_size=8,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = testset.to_pandas()[\"question\"].to_list()\n",
    "# ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./questions_answers/qa.csv\", delimiter=\";\")\n",
    "questions = df[\"question\"].tolist()\n",
    "ground_truth = df[\"ground_truth\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "for query in questions:\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(rag_chain.invoke(query))\n",
    "    data[\"contexts\"].append(\n",
    "        [doc.page_content for doc in retriever.invoke(query)]\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_entry = {\n",
    "    \"question\": data[\"question\"][0],\n",
    "    \"answer\": data[\"answer\"][0],\n",
    "    \"contexts\": data[\"contexts\"][0],\n",
    "    \"ground_truth\": data[\"ground_truth\"][0],\n",
    "}\n",
    "first_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        context_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    llm=model,\n",
    "    embeddings=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "heatmap_data = df[\n",
    "    [\n",
    "        \"context_relevancy\",\n",
    "        \"context_precision\",\n",
    "        \"context_recall\",\n",
    "        \"faithfulness\",\n",
    "        \"answer_relevancy\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\"green_red\", [\"red\", \"green\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", linewidths=0.5, cmap=cmap)\n",
    "\n",
    "plt.yticks(ticks=range(len(df[\"question\"])), labels=df[\"question\"], rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
